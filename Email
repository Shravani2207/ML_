ASSIGNMENT NO.2
Classify the email using the binary classification method. Email Spam detection has
two states: a) Normal State – Not Spam, b) Abnormal State – Spam. Use K-Nearest
Neighbors and Support Vector Machine for classification. Analyze their performance.
Dataset link: The emails.csv dataset on the Kaggle
https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics
df=pd.read_csv('emails.csv')
df.head()
df.columns
df.isnull().sum()
df.dropna(inplace = True)
df.drop(['Email No.'],axis=1,inplace=True)
X = df.drop(['Prediction'],axis = 1)
y = df['Prediction']
from sklearn.preprocessing import scale
X = scale(X)
# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print("Prediction",y_pred)
print("KNN accuracy = ",metrics.accuracy_score(y_test,y_pred))
print("Confusion matrix",metrics.confusion_matrix(y_test,y_pred))
# cost C = 1
model = SVC(C = 1)
# fit
model.fit(X_train, y_train)
# predict
y_pred = model.predict(X_test)
metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)
print("SVM accuracy = ",metrics.accuracy_score(y_test,y_pred))

short notes about intent and typical outputs where useful.

import pandas as pd
Imports the pandas library and gives it the alias pd. Pandas is used for reading, manipulating, and analyzing tabular data (DataFrames).
import numpy as np
Imports NumPy as np. NumPy provides fast numerical arrays and math functions used throughout data processing and ML.
import seaborn as sns
Imports Seaborn (aliased sns), a statistical data-visualization library built on Matplotlib; useful for nicer plots and heatmaps.
import matplotlib.pyplot as plt
Imports Matplotlib’s pyplot module as plt, the basic plotting API for creating charts.
%matplotlib inline
Jupyter magic that makes Matplotlib plots display directly inside the notebook output cells (inline). Works only in Jupyter notebooks / IPython.
import warnings
warnings.filterwarnings('ignore')
Imports Python’s warnings module and then disables (hides) warning messages so they don’t clutter notebook output. Use with care—warnings can be informative.
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics
train_test_split: utility to split dataset into train and test subsets.
SVC: Support Vector Classifier (SVM implementation) from scikit-learn.
metrics: module containing functions to evaluate model performance (accuracy, confusion matrix, etc.).
________________________________________
df = pd.read_csv('emails.csv')
Reads the CSV file emails.csv from the working directory into a pandas DataFrame called df. If the file isn't present you'll get a FileNotFoundError.
df.head()
Displays the first 5 rows of df. Useful for quickly inspecting column names and example records.
df.columns
Returns the list (Index) of column names in the DataFrame.
df.isnull().sum()
Checks for missing values: returns the number of nulls in each column. Good for spotting incomplete data.
df.dropna(inplace=True)
Drops any rows that contain at least one NaN value. inplace=True modifies df directly. After this line there should be no missing values left.
df.drop(['Email No.'], axis=1, inplace=True)
Removes the column named 'Email No.' from df. axis=1 means drop a column; inplace=True updates df in place. This column was likely just an index/ID and not useful for prediction.
X = df.drop(['Prediction'], axis=1)
y = df['Prediction']
X becomes the feature matrix: all columns except 'Prediction' (i.e., the inputs).
y becomes the target vector: the 'Prediction' column (what you want to predict, e.g., spam vs not-spam).
This is the standard features/labels separation for supervised learning.
________________________________________
from sklearn.preprocessing import scale
X = scale(X)
Imports the scale function (standardizes features).
X = scale(X) standardizes each feature to have mean 0 and variance 1 (z-score scaling). This is important for distance-based models (KNN, SVM) so features are on the same scale.
Note: scale returns a NumPy array (not a DataFrame).
________________________________________
# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
Splits the data into training and testing subsets:
test_size=0.3 reserves 30% of data for testing and 70% for training.
random_state=42 fixes the random seed so the split is reproducible.
Result: four variables — training features/labels and testing features/labels.
________________________________________
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
Imports the K-Nearest Neighbors classifier.
Creates an instance knn with n_neighbors=7, meaning the classifier will look at the 7 nearest neighbors to decide the class of a point.
knn.fit(X_train, y_train)
Fits the KNN classifier on the training data. For KNN, .fit() stores the training instances (KNN is a lazy learner).
y_pred = knn.predict(X_test)
Uses the trained KNN to predict labels for the test features. y_pred is the array of predicted labels.
print("Prediction", y_pred)
Prints the predicted labels array to the console.
print("KNN accuracy = ", metrics.accuracy_score(y_test, y_pred))
Computes and prints the classification accuracy of the KNN model on test data: fraction of correct predictions (correct / total).
print("Confusion matrix", metrics.confusion_matrix(y_test, y_pred))
Computes and prints the confusion matrix for KNN: a 2×2 (for binary) or NxN (for multi-class) table showing true vs predicted class counts (TP, FP, TN, FN for binary).
________________________________________
# cost C = 1
model = SVC(C = 1)
Creates a Support Vector Classifier with regularization parameter C = 1. C controls the trade-off between a smooth decision boundary and classifying training points correctly; larger C → less regularization (tries harder to classify training points correctly).
# fit
model.fit(X_train, y_train)
Trains the SVM model on the training data. This builds the support vectors and decision boundary.
# predict
y_pred = model.predict(X_test)
Predicts labels for the test set with the trained SVM. Note: this reuses the variable name y_pred and overwrites the previous KNN predictions.
metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)
Computes the confusion matrix for the SVM predictions. In your code you did not print it—this line returns the matrix but won’t display unless printed or in an interactive cell where return values are shown.
print("SVM accuracy = ", metrics.accuracy_score(y_test, y_pred))
Computes and prints the classification accuracy of the SVM on the test set.
________________________________________
Q1. What is the aim of this project?
A: To predict whether an email is spam or not using machine learning algorithms like K-Nearest Neighbors (KNN) and Support Vector Machine (SVM).
Q2. What dataset did you use?
A: The dataset is emails.csv, which contains various features extracted from emails (like word frequencies, character frequencies, etc.) and a target column Prediction (1 for spam, 0 for not spam).
Q3. What kind of machine learning problem is this?
A: It’s a classification problem (binary classification: spam vs. not spam).
Q4. Why do we import libraries like pandas and numpy?
pandas is used for reading and handling datasets.
numpy is used for numerical operations on arrays.
Q5. What is the use of df.isnull().sum()?
A: It checks for missing values (nulls) in the dataset.
Q6. What does df.dropna(inplace=True) do?
A: It removes all rows that contain any missing (NaN) values, directly updating the dataframe.
Q7. Why did you drop the column ‘Email No.’?
A: Because it’s just an identifier and doesn’t affect prediction; it’s irrelevant for the learning process.
Q8. What are X and y?
X → input features (independent variables)
y → target label (dependent variable, i.e., spam/not spam)
Q9. Why did you use scale(X)?
A: To standardize the dataset so that all features have a mean of 0 and standard deviation of 1.
This helps algorithms like KNN and SVM which are sensitive to feature scales.
Q10. What does train_test_split do?
A: It divides the dataset into two parts — training data (used to train the model) and testing data (used to evaluate it).
Q11. Why did you use test_size=0.3?
A: 30% of the data is reserved for testing and 70% is used for training to check model performance on unseen data.
Q12. What is the purpose of random_state=42?
A: It ensures that the data is split in the same way every time the code is run (for reproducibility).
Q13. What is KNN?
A: KNN (K-Nearest Neighbors) is a lazy learning algorithm that classifies a data point based on how its neighbors are classified.
Q14. Why did you choose n_neighbors=7?
A: It means the algorithm looks at the 7 nearest neighbors to decide the class. The value of k is chosen through experimentation to get good accuracy.
Q15. What does knn.fit(X_train, y_train) do?
A: It stores the training data so that KNN can use it to find nearest neighbors during prediction.
Q16. What is happening in y_pred = knn.predict(X_test)?
A: The trained KNN model predicts whether each email in the test set is spam or not.
Q17. How is accuracy calculated?
A: Accuracy = (Number of correct predictions) / (Total predictions)
Q18. What is a confusion matrix?
A: It shows model performance by comparing actual vs predicted labels:
True Positive (TP)
False Positive (FP)
True Negative (TN)
False Negative (FN)
(SVM)
Q19. What is SVM?
A: Support Vector Machine is a supervised learning algorithm that finds the best hyperplane (boundary) to separate classes with the maximum margin.
Q20. What does C=1 mean?
A: C is the regularization parameter.
Small C → smoother boundary, allows some misclassifications.
Large C → tries to classify all points correctly, risk of overfitting.
Here C=1 is a balanced choice.
Q21. What is model.fit(X_train, y_train) doing?
A: It trains the SVM model on the training dataset to find the optimal separating hyperplane.
Q22. What is the purpose of model.predict(X_test)?
A: It predicts the class labels for test data using the trained SVM model.
Q23. What is the difference between KNN and SVM?
Feature	KNN	SVM
Type	Lazy learner	Eager learner
Decision boundary	Based on nearest neighbors	Based on hyperplane
Sensitive to scale	Yes	Yes
Training time	Fast	Slower
Prediction time	Slow	Fast
Q24. How do you compare the models?
A: By checking their accuracy scores and confusion matrices. The model with higher accuracy and lower misclassifications is better.
Q25. What is accuracy in your result?
A: (You’ll answer based on your actual output — e.g., “KNN accuracy was 91%, SVM accuracy was 94%.”)
Q26. Which model performed better and why?
A: Usually SVM performs better because it creates an optimal decision boundary that separates classes more effectively.
Q27. What are some other evaluation metrics you could use?
A: Precision, Recall, F1-score, ROC curve, AUC score.
Q28. What are supervised and unsupervised learning?
Supervised: Data has labeled outputs (e.g., spam/not spam).
Unsupervised: Data has no labels (e.g., clustering, pattern discovery).
Q29. Why is feature scaling important?
 Because algorithms like KNN and SVM compute distances or margins — if one feature has larger numerical range, it can dominate others.
Q30. What is overfitting?
When a model performs well on training data but poorly on unseen data — it memorizes instead of generalizing.
Q31. What would happen if you didn’t scale the data?
 KNN and SVM results would be poor because features with larger numeric ranges would bias the model.
Q32. What happens if you change test_size to 0.5?
 Half of the data will be used for testing; model may train less effectively due to less training data.
Q33. What does inplace=True do in pandas?
 It updates the dataframe directly without creating a copy.
Q34. Why do we use random_state?
To get the same random split each time you run the code (reproducibility).
Q35. What happens if you increase the number of neighbors in KNN?
 The model becomes smoother (less sensitive to noise) but may lose accuracy if too large.
________________________________________
Q1. What is the title of your assignment?
A: “Classify the email using the binary classification method. Email spam detection using K-Nearest Neighbors and Support Vector Machine.”
Q2. What are the two states in this classification problem?
Normal State → Not Spam (0)
Abnormal State → Spam (1)
Q3. What is the main aim of this experiment?
 To classify emails as spam or not spam using binary classification with KNN and SVM, and compare their performance.
Q4. What is binary classification?
 Binary classification is a type of machine learning where the output can have only two possible classes (e.g., spam/not spam, true/false, yes/no).
Q5. What dataset are you using?
The dataset is from Kaggle:
https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv
Q6. How many rows and columns does the dataset have?
 5172 rows and 3002 columns.
Q7. What does the first column in the dataset represent?
 The first column represents the email name or identifier.
Q8. What does the last column represent?
 The last column contains the labels for prediction:
1 → Spam
0 → Not Spam
Q9. What do the remaining columns represent?
 They represent the frequencies of the 3000 most common words appearing in the emails (after cleaning and removing non-alphabetic characters).
Q10. How is the dataset stored?
 All 5172 email records are stored in a compact dataframe instead of separate text files.
Q11. What is the objective of this assignment?
To classify emails using binary classification and implement a spam detection technique using KNN and SVM, analyzing and comparing their performance.
Q12. What are the prerequisites for this assignment?
Basic knowledge of Python
Familiarity with machine learning libraries such as pandas, numpy, and sklearn
Q13. What is data preprocessing?
 Data preprocessing is the process of cleaning and preparing raw data to make it suitable for machine learning.
It includes handling missing values, scaling features, and encoding categorical data.
Q14. Why is data preprocessing necessary?
To remove noise and inconsistencies
To handle missing or invalid values
To make the dataset suitable for model training
To improve accuracy and efficiency of the model
Q15. What are the common steps in data preprocessing?
Getting the dataset
Importing libraries
Importing dataset
Finding and handling missing data
Encoding categorical data
Splitting dataset into training and testing sets
Feature scaling
Q16. What is feature scaling and why is it important?
Feature scaling ensures all features have similar ranges, preventing large-scale features from dominating the learning algorithm. It is especially important for KNN and SVM.
Q17. What is KNN?
 KNN (K-Nearest Neighbors) is a supervised learning algorithm that classifies data points based on the majority label of their nearest neighbors.
Q18. How does KNN work?
Choose the number of neighbors (k).
Measure the distance between the test point and all training points.
Select the k nearest points.
Assign the class most common among those neighbors.
Q19. What is the advantage of KNN?
Simple to implement
No model training required (lazy learner)
Q20. What is a disadvantage of KNN?
Slow prediction for large datasets
Sensitive to feature scaling and noise
Q21. What is SVM?
 SVM (Support Vector Machine) is a supervised learning algorithm that finds the best decision boundary (hyperplane) to separate different classes.
Q22. What is the purpose of the hyperplane in SVM?
 The hyperplane is the line or plane that separates different classes in the feature space with the maximum possible margin.
Q23. What are support vectors?
 They are the data points closest to the hyperplane that help define its position and orientation.
Q24. What is the kernel function in SVM?
 A kernel function transforms data into a higher dimension to make it easier to separate linearly. Common kernels are linear, polynomial, and RBF.
Q25. What does the parameter C control in SVM?
C is the regularization parameter.
Low C allows more misclassifications (simpler boundary).
High C tries to classify all points correctly (risk of overfitting).
Q26. What is the purpose of splitting the dataset?
To train the model on one part of the data (training set) and test it on unseen data (testing set) to evaluate performance.
Q27. What percentage of data is usually used for testing?
 Commonly 20–30% of the total dataset is used for testing.
Q28. What evaluation metrics are used in this project?
Accuracy score
Confusion matrix
Possibly Precision, Recall, and F1-score
Q29. How can you compare KNN and SVM performance?
 By comparing their accuracy and confusion matrix results. The model with higher accuracy and fewer misclassifications performs better.
Q30. Which model generally performs better for spam detection?
 SVM often performs better as it efficiently handles high-dimensional data like text features.
Q31. What type of learning is used in this project?
 Supervised learning, because the dataset has predefined labels (spam or not spam).
Q32. What is the shape of your dataset after preprocessing?
 (5172, 3002) — 5172 rows and 3002 columns.
Q33. What is the difference between training accuracy and testing accuracy?
Training accuracy → model performance on known data
Testing accuracy → model performance on unseen data (actual measure of generalization)
Q34. What happens if data preprocessing is skipped?
 The model accuracy decreases due to noise, inconsistent formats, or unscaled features.
Q35. Why is this assignment important?
 It demonstrates practical implementation of machine learning algorithms (KNN and SVM) in real-world text classification.
________________________________________

